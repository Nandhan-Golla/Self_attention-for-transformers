The above mentioned file contains the dive into self attention mechanism / scaled dot product mechanism
for transformers it contains the detailed view with sample code implemented in numpy
